{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878e3c1c-b050-447d-a4d5-1a4d75fa0c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
      "     -------------------------------------- 278.2/278.2 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-3.17.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a21906d6-6941-4a29-bb10-8f94b9497731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from dotenv import load_dotenv , find_dotenv\n",
    "# load_dotenv(find-dotenv(), override = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a772ce-41a6-46c0-a7a9-347067f0ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    print(f'loading {file}')\n",
    "    loader = PyPDFLoader(file)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "550e66cd-6017-4d75-bef9-ea7300cfb824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading C:\\Users\\DELL\\Desktop\\jupyter_ai\\cs224n-self-attention-transformers-2023_draft.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document(r'C:\\Users\\DELL\\Desktop\\jupyter_ai\\cs224n-self-attention-transformers-2023_draft.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b49bcf-b9cd-43fa-b1db-5f96ad410309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[draft] Note 10: Self-Attention & Transformers11Course Instructors: Christopher\n",
      "Manning, John Hewitt 2CS224n: Natural Language Processing with Deep Learning\n",
      "2Author: John Hewitt\n",
      "johnhew@cs.stanford.edu Winter 2023\n",
      "Summary. This note motivates moving away from recurrent archi-\n",
      "tectures in NLP , introduces self-attention, and builds a minimal self-\n",
      "attention-based neural architecture. Finally, it dives into the details of\n",
      "the Transformer architecture, a self-attention-based architecture that\n",
      "as of 2023 is ubiquitous in NLP research.\n",
      "Embedding sAdd P osition \n",
      "Embedding sMulti-Head \n",
      "A tt entionAdd & N ormF eed-F or w ar dAdd & N ormEmbedding sAdd P osition \n",
      "Embedding sBloc kMask ed Multi-\n",
      "Head A tt entionAdd & N ormMulti-Head \n",
      "A tt entionAdd & N ormF eed-F or w ar dLinearSoftmaxAdd & N orm\n",
      "Bloc kR epeat  f or number of \n",
      "encoder bloc ksR epeat  f or number of \n",
      "decoder bloc ks.\n",
      "\n",
      "A ttend onl y to output of \n",
      "last E ncoder Bloc k.Pr obabilities\n",
      "Decoder InputsE ncoder InputsT r ansf ormer E ncoder-Decoder\n",
      "1Neural architectures and their properties\n",
      "Progress in natural language processing is often accelerated by\n",
      "general-purpose techniques that perform better than earlier meth-\n",
      "ods across a wide range of settings. Sometimes, a new idea is\n",
      "proposed to solve old problems; other times, old techniques be-\n",
      "come newly relevant as data or computation power becomes newly\n",
      "available. A few examples of these include hidden Markov models\n",
      "[Baum and Petrie, 1966 ], conditional random fields [ Lafferty et al., 2001 ],\n",
      "recurrent neural networks [ Rumelhart et al., 1985 ], convolutional\n",
      "neural networks [ LeCun et al., 1989 ], and support vector machines\n",
      "[Cortes and Vapnik, 1995 ].\n",
      "In this section, we’ll discuss a bit about the neural modeling ap-\n",
      "proaches we’ve discussed in Cs 224n so far, and how their limitations\n",
      "(and changes in the world) inspired the modern (as of 2023 ) zeitgeist\n",
      "of self-attention and Transformer-based architectures.\n",
      "1.1Notation and basics\n",
      "Letw1:nbe a sequence, where each wi∈ V, a finite vocabulary. We’ll\n",
      "also overload w1:nto be a matrix of one-hot vectors, w1:n∈Rn×|V|.\n",
      "We’ll use w∈ V to represent an arbitrary vocabulary element, and\n",
      "wi∈ V to pick out a specific indexed element of a sequence w1:n.\n",
      "We’ll use the notation,\n",
      "wt∼softmax (f(w1:t−1)), ( 1)\n",
      "to mean that under a model, wt“is drawn from” the probability\n",
      "distribution defined by the right-hand-side of the tilde, ∼. So in this\n",
      "case, f(w1:t−1)should be in R|V|. When we use the softmax function\n",
      "(as above), we’ll use it without direct reference to the dimension\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c6ad046-192a-4619-93e9-ec915d19c853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d186423-7c02-4c1e-8619-87f43aa76cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------- 0\n",
      "[draft] Note 10: Self-Attention & Transformers11Course Instructors: Christopher\n",
      "Manning, John Hewitt 2CS224n: Natural Language Processing with Deep Learning\n",
      "2Author: John Hewitt\n",
      "johnhew@cs.stanford.edu Winter 2023\n",
      "Summary. This note motivates moving away from recurrent archi-\n",
      "tectures in NLP , introduces self-attention, and builds a minimal self-\n",
      "attention-based neural architecture. Finally, it dives into the details of\n",
      "the Transformer architecture, a self-attention-based architecture that\n",
      "as of 2023 is ubiquitous in NLP research.\n",
      "Embedding sAdd P osition \n",
      "Embedding sMulti-Head \n",
      "A tt entionAdd & N ormF eed-F or w ar dAdd & N ormEmbedding sAdd P osition \n",
      "Embedding sBloc kMask ed Multi-\n",
      "Head A tt entionAdd & N ormMulti-Head \n",
      "A tt entionAdd & N ormF eed-F or w ar dLinearSoftmaxAdd & N orm\n",
      "Bloc kR epeat  f or number of \n",
      "encoder bloc ksR epeat  f or number of \n",
      "decoder bloc ks.\n",
      "\n",
      "A ttend onl y to output of \n",
      "last E ncoder Bloc k.Pr obabilities\n",
      "Decoder InputsE ncoder InputsT r ansf ormer E ncoder-Decoder\n",
      "1Neural architectures and their properties\n",
      "Progress in natural language processing is often accelerated by\n",
      "general-purpose techniques that perform better than earlier meth-\n",
      "ods across a wide range of settings. Sometimes, a new idea is\n",
      "proposed to solve old problems; other times, old techniques be-\n",
      "come newly relevant as data or computation power becomes newly\n",
      "available. A few examples of these include hidden Markov models\n",
      "[Baum and Petrie, 1966 ], conditional random fields [ Lafferty et al., 2001 ],\n",
      "recurrent neural networks [ Rumelhart et al., 1985 ], convolutional\n",
      "neural networks [ LeCun et al., 1989 ], and support vector machines\n",
      "[Cortes and Vapnik, 1995 ].\n",
      "In this section, we’ll discuss a bit about the neural modeling ap-\n",
      "proaches we’ve discussed in Cs 224n so far, and how their limitations\n",
      "(and changes in the world) inspired the modern (as of 2023 ) zeitgeist\n",
      "of self-attention and Transformer-based architectures.\n",
      "1.1Notation and basics\n",
      "Letw1:nbe a sequence, where each wi∈ V, a finite vocabulary. We’ll\n",
      "also overload w1:nto be a matrix of one-hot vectors, w1:n∈Rn×|V|.\n",
      "We’ll use w∈ V to represent an arbitrary vocabulary element, and\n",
      "wi∈ V to pick out a specific indexed element of a sequence w1:n.\n",
      "We’ll use the notation,\n",
      "wt∼softmax (f(w1:t−1)), ( 1)\n",
      "to mean that under a model, wt“is drawn from” the probability\n",
      "distribution defined by the right-hand-side of the tilde, ∼. So in this\n",
      "case, f(w1:t−1)should be in R|V|. When we use the softmax function\n",
      "(as above), we’ll use it without direct reference to the dimension\n",
      "-------------------------------------------------------------------------------------- 1\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 2\n",
      "being normalized over, and it should be interpreted as follows. If Ais\n",
      "a tensor of shape Rℓ,d, the softmax is computed as follows:\n",
      "softmax (A)i,j=expAi,j\n",
      "∑d\n",
      "j′=1expAi,j′, ( 2)\n",
      "for all i∈1, . . . ,ℓ,j∈1, . . . , d, and similarly for tensors of more than\n",
      "two axes. That is, if we had a tensor B∈Rm,ℓ,d, we would define\n",
      "the softmax over the last dimension, similarly. At the risk of being\n",
      "verbose, we’ll write it out:\n",
      "softmax (B)q,i,j=expBq,i,j\n",
      "∑d\n",
      "j′=1expBq,i,j′. ( 3)\n",
      "In all of our methods, we’ll assume an embedding matrix, E∈\n",
      "Rd×|V|, mapping from the vocabulary space to the hidden dimensional-\n",
      "ity d , written as Ex∈Rd. Embedding definition\n",
      "The embedding Ewiof a token in sequence w1:nis what’s known\n",
      "as anon-contextual representation; despite wiappearing in a se-\n",
      "quence, the representation Ewiis independent of context. Since we’ll\n",
      "almost always be working on the embedded version of w1:n, we’ll\n",
      "letx=Ew, and x1:n=w1:nE⊤∈Rn×d. An overarching goal of\n",
      "the methods discussed in this note is to develop strong contextual\n",
      "representations of tokens; that is, a representation hithat represents\n",
      "wibut is a function of the entire sequence x1:n(or a prefix x1:i, as in\n",
      "the case of language modeling.). A non-contextual representation of\n",
      "a token xiof sequence x1:ndepends\n",
      "only on the identity of xi; a contextual\n",
      "representation of xidepends on the\n",
      "entire sequence (or a prefix x1:i.)1.2The default circa 2017 : recurrent neural networks\n",
      "General-purpose modeling techniques and representations have a\n",
      "long history in NLP , with individual techniques falling in and out of\n",
      "favor. Word embeddings, for example, have a much longer history\n",
      "than the word 2vec embeddings we studied in the first few lectures\n",
      "[Schütze, 1992 ]. Likewise, recurrent neural networks have a long\n",
      "and non-monotonic history in modeling problems [ Elman, 1990 ,\n",
      "Bengio et al., 2000 ]. By 2017 , however, the basic strategy to solve\n",
      "a natural language processing task was to begin with a recurrent\n",
      "neural network.\n",
      "We’ve gone over RNNs earlier in the course, but the general form\n",
      "bears repeating here. A simple form of RNN is as follows:\n",
      "ht=σ(Wht−1+Uxt), ( 4)Dependence on the sequence index\n",
      "where ht∈Rd,U∈Rd×d, and W∈Rd×d. By2017 , the intuition\n",
      "was that there were twofold issues with the recurrent neural net-\n",
      "work form, and they both had to do with the the depenence on the\n",
      "-------------------------------------------------------------------------------------- 2\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 3\n",
      "sequence index (often called the dependence on “time”) highlighted\n",
      "in Equation 4.\n",
      "Parallelization issues with dependence on the sequence index. Modern\n",
      "graphics processing units (GPUs) are excellent at crunching through\n",
      "a lot of simple operations (like addition) in parallel . For example,\n",
      "when I have a matrix A∈Rn×kand a matrix B∈Rk×d, a GPU is\n",
      "just blazing fast at computing AB∈Rn×d. The constraint of the\n",
      "operations occuring in parallel, however, is crucial – when computing\n",
      "ABthe simplest way, I’m performing a bunch of multiplies and then\n",
      "a bunch of sums, most of which don’t depend on the output of each\n",
      "other. However, in a recurrent neural network, when I compute\n",
      "h2=σ(Wh1+Ux2), ( 5)\n",
      "I can’t compute h2until I know the value of h1, so we can write it out\n",
      "as\n",
      "h2=σ(Wσ(Wh0+Ux1) +Ux2). ( 6)\n",
      "Likewise if I wanted to compute h3, I can’t compute it until I know\n",
      "h2, which I can’t compute until I know h1, etc. Visually, this looks\n",
      "like Figure 1. As the sequence gets longer, there is only so much I\n",
      "can parallelize the computation of the network on a GPU because\n",
      "of the number of serial dependencies. (Serial meaning one-after-the-\n",
      "other.)\n",
      "1 2 3 4 5\n",
      "0 0 0 0 0\n",
      "Zuko made his uncle teaFigure 1: A RNN unrolled in time.\n",
      "The rectangles are intermediate states\n",
      "of the RNN (e.g., the first row is the\n",
      "embedding layer, and the second row is\n",
      "the RNN hidden state at each time step)\n",
      "and the number in the rectangle is the\n",
      "number of serial operations that need to\n",
      "be performed before this intermediate\n",
      "state can be computed\n",
      "As GPUs (and later, other accelerators like Tensor Processing Units\n",
      "(TPUs) became more powerful and researchers wanted to take fuller\n",
      "advantage of them, this dependence in time became untenable.\n",
      "Linear interaction distance. A related issue with RNNs is the diffi-\n",
      "culty with which distant tokens in a sequence can interact with each\n",
      "other. By interact, we mean that the presence of one token (already\n",
      "observed in the past) gainfully affects the processing of another token.\n",
      "For example, in the sentence\n",
      "The chef 1who ran out of blackberries and went to the stores is 1\n",
      "-------------------------------------------------------------------------------------- 3\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 4\n",
      "the number of intermediate computations—matrix multiplies and\n",
      "nonlinearities, for example—that separate chef from isscales with the\n",
      "number of words between them. We visualize this in Figure 2.\n",
      "1 2 3 4 5\n",
      "0\n",
      "Zuko made his uncle teaFigure 2: A RNN unrolled in time.\n",
      "The rectangles are intermediate states\n",
      "of the RNN (e.g., the first row is the\n",
      "embedding layer, and the second row\n",
      "is the RNN hidden state at each time\n",
      "step) and the number in the rectangle\n",
      "is roughly the number of operations\n",
      "separating lexical information of the\n",
      "word teafrom each intermediate state.\n",
      "Intuitively, researchers believe there’s an issue with linear inter-\n",
      "action distance because it can be difficult for networks to precisely\n",
      "“recall” the presence of a word when a large number of operations\n",
      "occur after observing that word. This can make it difficult to learn\n",
      "how distant words should impact the representation of the current\n",
      "word.\n",
      "This notion of direct interaction between elements of a sequence\n",
      "might remind you of the attention mechanism [ Bahdanau et al., 2014 ]\n",
      "in machine translation. In that context, while generating a translation,\n",
      "we learned how to look back into the source sequence once per token\n",
      "of the translation. In this note, we’ll present an entire replacement\n",
      "for recurrent neural networks just based on attention. This will solve\n",
      "both the parallelization issues and the linear interaction distance\n",
      "issues with recurrent neural networks.\n",
      "2A minimal self-attention architecture\n",
      "Attention, broadly construed, is a method for taking a query, and\n",
      "softly looking up information in a key-value store by picking the\n",
      "value(s) of the key(s) most like the query. By “picking” and “most\n",
      "like,” we mean averaging overall values, putting more weight on\n",
      "those which correspond to the keys more like the query. In self-\n",
      "attention , we mean that we use the same elements to help us define\n",
      "the querys as we do the keys and values.\n",
      "In this section, we’ll discuss how to develop contextual representa-\n",
      "tions with methods wherein the main mechanism for contextualiza-\n",
      "tion is not recurrence, but attention.\n",
      "2.1The key-query-value self-attention mechanism\n",
      "There are many forms of self-attention; the form we’ll discuss here is\n",
      "currently the most popular. It’s called key-query-value self-attention.\n",
      "-------------------------------------------------------------------------------------- 4\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 5\n",
      "Consider a token xiin the sequence x1:n. From it, we define a\n",
      "query qi=Qxi, for matrix Q∈Rd×d. Then, for each token in the\n",
      "sequence xj∈ {x1. . . ,xn}, we define both a key and a value similarly,\n",
      "with two other weight matrices: kj=Kxj, and vj=VxjforK∈Rd×d\n",
      "and V∈Rd×d.\n",
      "Our contextual representation hiofxiis a linear combination (that\n",
      "is, a weighted sum) of the values of the sequence,\n",
      "hi=n\n",
      "∑\n",
      "j=1αijvj, ( 7)\n",
      "where the weights, these αijcontrol the strength of contribution of\n",
      "each vj. Going back to our key-value store analogy, the αijsoftly se-\n",
      "lects what data to look up. We define these weights by computing the\n",
      "affinities between the keys and the query, q⊤\n",
      "ikj, and then computing\n",
      "the softmax over the sequence:\n",
      "αij=exp(q⊤\n",
      "ikj)\n",
      "∑n\n",
      "j′=1exp(q⊤\n",
      "ikj′)(8)\n",
      "Intuitively, what we’ve done by this operation is take our element\n",
      "xiand look in its own sequence x1:nto figure out what information\n",
      "(in an informal sense,) from what other tokens, should be used in\n",
      "representing xiin context. The use of matrices K,Q,Vintuitively\n",
      "allow us to use different views of the xifor the different roles of\n",
      "key, query, and value. We perform this operation to build hifor all\n",
      "i∈ {1, . . . , n}.\n",
      "v 1 : n  =V x 1 : nk 1 : n  =Kx 1 : nq i  =Qx i(query)(weights)(weighted a verage)\n",
      "(ke y)(value)h i  =α i jv j\n",
      "αq iTk j  -> softmax∑scalarvectorSelf-Attention\n",
      "-------------------------------------------------------------------------------------- 5\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 6\n",
      "2.2Position representations\n",
      "Consider the sequence the oven cooked the bread so . This is a different\n",
      "sequence than the bread cooked the oven so , as you might guess. The for-\n",
      "mer sentence has us making delicious bread, and the latter we might\n",
      "interpret as the bread somehow breaking the oven. In a recurrent\n",
      "neural network, the order of the sequence defines the order of the\n",
      "rollout, so the two sequences have different representations. In the\n",
      "self-attention operation, there’s no built-in notion of order. The self-attention operation has no\n",
      "built-in notion of the sequence order.To see this, let’s take a look at self-attention on this sequence. We\n",
      "have a set of vectors x1:nforthe oven cooked the bread so , which we can\n",
      "write as\n",
      "x1:n=[xthe;xoven;xcooked ;xthe;xbread ;xso]∈R5×d(9)\n",
      "As an example, consider performing self-attention to represent the\n",
      "word soin context. The weights over the context are as follows,\n",
      "recalling that qi=Qxifor all words, and ki=Kxilikewise:\n",
      "αso=softmax\u0010h\n",
      "q⊤\n",
      "sokthe;q⊤\n",
      "sokoven;q⊤\n",
      "sokcooked ;q⊤\n",
      "sokthe;q⊤\n",
      "sokbread ;q⊤\n",
      "soksoi\u0011\n",
      "(10)\n",
      "So, the weight αso,0, the amount that we look up the first word, (by\n",
      "writing out the softmax) is,\n",
      "αso,0=exp(q⊤\n",
      "sokthe)\n",
      "exp(q⊤sokthe) +· · ·+exp(q⊤sokbread). ( 11)\n",
      "So,α∈R5are our weights, and we compute the weighted average in\n",
      "Equation 7with these weights to compute hso.\n",
      "For the reordered sentence the bread cooked the oven , note that αso,0\n",
      "is identical. The numerator hasn’t changed, and the denominator\n",
      "hasn’t changed; we’ve just rearranged terms in the sum. Likewise\n",
      "forαso,bread and αso,oven , you can compute that they too are identical\n",
      "independent of the ordering of the sequence. This all comes back\n",
      "down to the two facts that ( 1) the representation of xis not position-\n",
      "dependent; it’s just Ewfor whatever word w, and ( 2) there’s no\n",
      "dependence on position in the self-attention operations. Non-contextual embedded words\n",
      "xi=Ewihave no dependence on the\n",
      "word’s position in a sequence w1:n; only\n",
      "on the identity of the word in V. Position representation through learned embeddings. To represent po-\n",
      "sition in self-attention, you either need to ( 1) use vectors that are\n",
      "already position-dependent as inputs, or ( 2) change the self-attention\n",
      "operation itself. One common solution is a simple implementation\n",
      "of (1). We posit a new parameter matrix, P∈RN×d, where Nis\n",
      "themaximum length of any sequence that your model will be able to\n",
      "process.\n",
      "-------------------------------------------------------------------------------------- 6\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 7\n",
      "We then simply add embedded representation of the position of a\n",
      "word to its word embedding:\n",
      "˜xi=Pi+xi (12)\n",
      "and perform self-attention as we otherwise would. Now, the self-\n",
      "attention operation can use the embedding Pito look at the word at\n",
      "position idifferently than if that word were at position j. This is done,\n",
      "e.g., in the BERT paper [ Devlin et al., 2019 ] (which we go over later in\n",
      "the course.)\n",
      "Position representation through changing αdirectly. Instead of chang-\n",
      "ing the input representation, another thing we can do is change\n",
      "the form of self-attention to have a built-in notion of position. One\n",
      "intuition is that all else held equal, self-attention should look at\n",
      "“nearby” words more than “far” words. Attention with Linear Biases\n",
      "[Press et al., 2022 ] is one implementation of this idea. One implemen-\n",
      "tation of this would be as follows:\n",
      "αi=softmax (k1:nqi+ [−i, . . . ,−1, 0,−1, . . . ,−(n−i)]), (13)\n",
      "where k1:nqi∈Rnare the original attention scores, and the bias\n",
      "we add makes attention focus more on nearby words than far away\n",
      "words, all else held equal. In some sense, it’s odd that this works; but\n",
      "interesting!\n",
      "2.3Elementwise nonlinearity\n",
      "Imagine if we were to stack self-attention layers. Would this be\n",
      "sufficient for a replacement for stacked LSTM layers? Intuitively,\n",
      "there’s one thing that’s missing: the elementwise nonlinearities that\n",
      "we’ve come to expect in standard deep learning architectures. In fact,\n",
      "if we stack two self-attention layers, we get something that looks a lot\n",
      "like a single self-attention layer:\n",
      "oi=n\n",
      "∑\n",
      "j=1αijV(2) \n",
      "n\n",
      "∑\n",
      "k=1αjkV(1)xk!\n",
      "(14)\n",
      "=n\n",
      "∑\n",
      "k=1 \n",
      "αjkn\n",
      "∑\n",
      "j=1αij!\n",
      "V(2)V(1)xk (15)\n",
      "=n\n",
      "∑\n",
      "k=1α∗\n",
      "ijV∗xk, ( 16)\n",
      "where α∗\n",
      "ij=\u0010\n",
      "αjk∑n\n",
      "j=1αij\u0011\n",
      ", and V∗=V(2)V(1). So, this is just a linear\n",
      "combination of a linear transformation of the input, much like a\n",
      "single layer of self-attention! Is this good enough?3 3This question ends up having a\n",
      "nuanced answer that’s out-of-scope for\n",
      "this note; ask me if you’re interested in\n",
      "knowing more!\n",
      "-------------------------------------------------------------------------------------- 7\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 8\n",
      "In practice, after a layer of self-attention, it’s common to apply\n",
      "feed-forward network independently to each word representation:\n",
      "hFF=W2ReLU (W1hself-attention +b1) +b2, ( 17)\n",
      "where often, W1∈R5d×d, and W2∈Rd×5d. That is, the hidden\n",
      "dimension of the feed-forward network is substantially larger than\n",
      "the hidden dimension of the network, d—this is done because this\n",
      "matrix multiply is an efficiently parallelizable operation, so it’s an\n",
      "efficient place to put a lot of computation and parameters.\n",
      "2.4Future masking\n",
      "When performing language modeling like we’ve seen in this course\n",
      "(often called autoregressive modeling), we predict a word given all\n",
      "words so far:\n",
      "wt∼softmax (f(w1:t−1)). ( 18)\n",
      "where fis function to map a sequence to a vector in R|V|.\n",
      "One crucial aspect of this process is that we can’t look at the future\n",
      "when predicting it—otherwise the problem becomes trivial. This idea\n",
      "is built-in to unidirectional RNNs. If we want to use an RNN for the\n",
      "function f, we can use the hidden state for word wt−1:\n",
      "wt∼softmax (ht−1E) (19)\n",
      "ht−1=σ(Wht−2+Uxt−1), ( 20)\n",
      "and by the rollout of the RNN, we haven’t looked at the future. (In\n",
      "this case, the future is all the words wt, . . . , wn.)\n",
      "In a Transformer, there’s nothing explicit in the self-attention\n",
      "weight αthat says not to look at indices j>iwhen representing\n",
      "token i. In practice, we enforce this constraint simply adding a large\n",
      "negative constant to the input to the softmax (or equivalently, setting\n",
      "αij=0 where j>i.)4 4It might seem like one should use −∞\n",
      "as the constant, to “really” ensure that\n",
      "you can’t see the future. However, this\n",
      "is not done; a modest constant within\n",
      "even the float range of the ‘float 16‘\n",
      "encoding is used instead, like −105.\n",
      "Using infinity can lead to NaNs and\n",
      "it’s sort of undefined how each library\n",
      "should treat infinite inputs, so we tend\n",
      "to avoid using it. And because of finite\n",
      "precision, a large enough negative\n",
      "constant will still set the attention\n",
      "weight to exactly zero.αij,masked =\n",
      "\n",
      "αijj≤i\n",
      "0 otherwise(21)\n",
      "In a diagram, it looks like Figure 3.\n",
      "2.5Summary of a minimal self-attention architecture\n",
      "Our minimal self-attention architecture has ( 1) the self-attention\n",
      "operation, ( 2) position representations, ( 3) elementwise nonlinearities,\n",
      "and ( 4) future masking (in the context of language modeling.)\n",
      "-------------------------------------------------------------------------------------- 8\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 9\n",
      "Zuko made his uncle tea\n",
      "Zuko\n",
      "made\n",
      "his\n",
      "uncle\n",
      "tea−∞−∞\n",
      "−∞−∞\n",
      "−∞\n",
      "−∞−∞\n",
      "−∞\n",
      "−∞\n",
      "−∞Figure 3: Diagram of autoregressive\n",
      "future masking in self-attention. Words\n",
      "in each row have words in the future\n",
      "masked out (e.g., “Zuko” can only\n",
      "attend to “Zuko”, while “made” can\n",
      "attend to “Zuko” and “made”.)\n",
      "Intuitively, these are the biggest components to understand. How-\n",
      "ever, as of 2023 , by far the most-used architecture in NLP is called\n",
      "theTransformer , introduced by [ Vaswani et al., 2017 ], and it contains\n",
      "a number of components that end up being quite important. So now\n",
      "we’ll get into the details of that architecture.\n",
      "3The Transformer\n",
      "The Transformer is an architecture based on self-attention that con-\n",
      "sists of stacked Blocks , each of which contains self-attention and feed-\n",
      "forward layers, and a few other components we’ll discuss. If you’d\n",
      "like to take a peek for intuition, we have a diagram of a Transformer\n",
      "language model architecture in Figure 4. The components we haven’t\n",
      "gone over are multi-head self-attention, layer normalization ,resid-\n",
      "ual connections , and attention scaling —and of course, we’ll discuss\n",
      "how these components are combined to form the Transformer.\n",
      "Embedding sAdd P osition \n",
      "Embedding sMask ed Multi-\n",
      "Head A tt entionAdd & N ormF eed-F or w ar dAdd & N orm\n",
      "Bloc kLinearSoftmaxR epeat  f or number of \n",
      "encoder bloc ksPr obabilities\n",
      "Decoder InputsT r ansf ormer Decoder\n",
      "Figure 4: Diagram of the Transformer\n",
      "Decoder (without corresponding\n",
      "Encoder, and so no cross-attention.3.1Multi-head Self-Attention\n",
      "Intuitively, a single call of self-attention is best at picking out a single\n",
      "value (on average) from the input value set. It does so softly, by\n",
      "averaging over all of the values, but it requires a balancing game in\n",
      "the key-query dot products in order to carefully average two or more\n",
      "things. In Assignment 5, you’ll work through a bit of this intuition\n",
      "more carefully. What we’ll present now, multi-head self-attention ,\n",
      "intuitively applies self-attention multiple times at once, each with\n",
      "different key, query, and value transformations of the same input,\n",
      "and then combines the outputs.\n",
      "For an integer number of heads k, we define matrices K(ℓ),Q(ℓ),V(ℓ)∈\n",
      "Rd×d/kforℓin{1, . . . , k}. (We’ll see why we have the dimensionality\n",
      "reduction to d/ksoon.) These our the key, query, and value matrices\n",
      "for each head. Correspondingly, we get keys, queries, and values\n",
      "k(ℓ)\n",
      "1:n,q(ℓ)\n",
      "1:n,v(ℓ)\n",
      "1:n, as in single-head self-attention.\n",
      "-------------------------------------------------------------------------------------- 9\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 10\n",
      "We then perform self-attention with each head:\n",
      "h(ℓ)\n",
      "i=n\n",
      "∑\n",
      "j=1α(ℓ)\n",
      "ijv(ℓ)\n",
      "j(22)\n",
      "α(ℓ)\n",
      "ij=exp(q(ℓ)⊤\n",
      "ik(ℓ)\n",
      "j)\n",
      "∑n\n",
      "j′=1exp(q(ℓ)⊤\n",
      "ik(ℓ)\n",
      "j′)(23)\n",
      "Note that the output h(ℓ)\n",
      "iof each head is in reduced dimension d/k.\n",
      "Finally, we define the output of multi-head self-attention as a linear\n",
      "transformation of the concatenation of the head outputs, letting\n",
      "O∈Rd×d:\n",
      "hi=Oh\n",
      "v(1)\n",
      "i;· · ·;v(k)\n",
      "ii\n",
      ", ( 24)\n",
      "where we concatenate the head outputs each of dimensionality d×d/k\n",
      "at their second axis, such that their concatenation has dimension d×d.\n",
      "Sequence-tensor form. To understand why we have the reduced\n",
      "dimension of each head output, it’s instructive to get a bit closer to\n",
      "how multi-head self-attention is implemented in code. In practice,\n",
      "multi-head self-attention is no more expensive than single-head\n",
      "due to the low-rankness of the transformations we apply.\n",
      "For a single head, recall that x1:nis a matrix in Rn×d. Then we can\n",
      "compute our value vectors as a matrix as x1:nV, and likewise our\n",
      "keys and queries x1:nKandx1:nQ, all matrices in Rn×d. To compute\n",
      "self-attention, we can compute our weights in matrix operations:\n",
      "α=softmax (x1:nQK⊤x⊤\n",
      "1:n)∈Rn×n(25)\n",
      "and then compute the self-attention operation for all x1:nvia:\n",
      "h1:n=softmax (x1:nQK⊤x⊤\n",
      "1:n)x1:nV∈Rn×d. ( 26)\n",
      "Here’s a diagram showing the matrix ops:\n",
      "x1 : n Q( x1 : n K)Tn=dsoftmaxnnα\n",
      "When we perform multi-head self-attention in this matrix form,\n",
      "we first reshape x1:nQ,x1:nK, and x1:nVeach into a matrix of shape\n",
      "Rn,k,d/k, splitting the model dimensionality into two axes, for the\n",
      "number of heads and the number of dimensions per head. We can\n",
      "-------------------------------------------------------------------------------------- 10\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 11\n",
      "then transpose the matrices to Rk,n,d/k, which intuitively should look\n",
      "likeksequences of length nand dimensionality d/k. This allows\n",
      "us to perform the batched softmax operation in parallel across the\n",
      "heads, using the number of heads kind of like a batch axis (and\n",
      "indeed in practice we’ll also have a separate batch axis.) So, the\n",
      "total computation (except the last linear transformation to combine\n",
      "the heads) is the same, just distributed across the (each lower-rank)\n",
      "heads. Here’s a diagram like the single-head diagram, demonstrating\n",
      "how the multi-head operation ends up much like the single-head\n",
      "operation:\n",
      "r eshape(x 1 : n Q)r eshape((x 1 : n K)T)n=d / ksoftmaxnknα\n",
      "3.2Layer Norm\n",
      "One important learning aid in Transformers is layer normalization\n",
      "[Ba et al., 2016 ]. The intuition of layer norm is to reduce uninforma-\n",
      "tive variation in the activations at a layer, providing a more stable\n",
      "input to the next layer. Further work shows that this may be most\n",
      "useful not in normalizing the forward pass, but actually in improving\n",
      "gradients in the backward pass [Xu et al., 2019 ].\n",
      "To do this, layer norm ( 1) computes statistics across the activations\n",
      "at a layer to estimate the mean and variance of the activations, and ( 2)\n",
      "normalizes the activations with respect to those estimates, while ( 3)\n",
      "optionally learning (as parameters) an elementwise additive bias and\n",
      "multiplicative gain by which to sort of de-normalize the activations\n",
      "in a predictable way. The third part seems not to be crucial, and may\n",
      "even be harmful [Xu et al., 2019 ], so we omit it in our presentation.\n",
      "One question to ask when understanding how layer norm affects\n",
      "a network is, “computing statistics over what ?” That is, what consti-\n",
      "tutes a layer? In Transformers, the answer is always that statistics\n",
      "computed independently for a single index into the sequence length\n",
      "(and a single example in the batch) and shared across the dhidden\n",
      "dimensions. Put another way, the statistics for the token at index i\n",
      "won’t affect the token at index j̸=i.\n",
      "So, we compute the statistics for a single index i∈ {1, . . . , n}as\n",
      "ˆµi=1\n",
      "dd\n",
      "∑\n",
      "j=1hij ˆσi=vuut1\n",
      "dd\n",
      "∑\n",
      "j=1(hij−µi)2, ( 27)\n",
      "-------------------------------------------------------------------------------------- 11\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 12\n",
      "where (as a reminder), ˆµiand ˆσiare scalars, and we compute the\n",
      "layer norm as\n",
      "LN(hi) =hi−ˆµi\n",
      "ˆσi, ( 28)\n",
      "where we’ve broadcasted the ˆµiand ˆσiacross the ddimensions of\n",
      "hi. Layer normalization is a great tool to have in your deep learning\n",
      "toolbox more generally.\n",
      "3.3Residual Connections\n",
      "Residual connections simply add the input of a layer to the output of\n",
      "that layer:\n",
      "fresidual (h1:n) =f(h1:n) +h1:n, ( 29)\n",
      "the intuition being that ( 1) the gradient flow of the identity function\n",
      "isgreat (the local gradient is 1everywhere!) so the connection allows\n",
      "for learning much deeper networks, and ( 2) it is easier to learn the\n",
      "difference of a function from the identity function than it is to learn\n",
      "the function from scratch. As simple as these seem, they’re massively\n",
      "useful in deep learning, not just in Transformers!\n",
      "Add & Norm. In the Transformer diagrams you’ll see, including Fig-\n",
      "ure4, the application of layer normalization and residual connection\n",
      "are often combined in a single visual block labeled Add & Norm . Such\n",
      "a layer might look like:\n",
      "hpre-norm =f(LN(h)) +h, ( 30)\n",
      "where fis either a feed-forward operation or a self-attention opera-\n",
      "tion, (this is known as pre-normalization ), or like:\n",
      "hpost-norm =LN(f(h) +h), ( 31)\n",
      "which is known as post-normalization . It turns out that the gradients\n",
      "ofpre-normalization are much better at initialization, leading to\n",
      "much faster training [Xiong et al., 2020 ].\n",
      "3.4Attention logit scaling\n",
      "Another trick introduced in [ Vaswani et al., 2017 ] they dub scaled dot\n",
      "product attention . The dot product part comes from the fact that we’re\n",
      "computing dot products q⊤\n",
      "ikj. The intuition of scaling is that, when\n",
      "the dimensionality dof the vectors we’re dotting grows large, the dot\n",
      "product of even random vectors (e.g., at initialization) grows roughly\n",
      "as√\n",
      "d. So, we normalize the dot products by√\n",
      "dto stop this scaling:\n",
      "α=softmax (x1:nQK⊤x⊤\n",
      "1:n√\n",
      "d)∈Rn×n(32)\n",
      "-------------------------------------------------------------------------------------- 12\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 13\n",
      "3.5Transformer Encoder\n",
      "A Transformer Encoder takes a single sequence w1:n, and performs\n",
      "no future masking. It embeds the sequence with Eto make x1:n, adds\n",
      "the position representation, and then applies a stack of independently\n",
      "parameterized Encoder Blocks , each of which consisting of ( 1) multi-\n",
      "head attention and Add & Norm, and ( 2) feed-forward and Add &\n",
      "Norm. So, the output of each Block is the input to the next. Figure 5\n",
      "presents this.\n",
      "Embedding sAdd P osition \n",
      "Embedding sMulti-Head \n",
      "A tt entionAdd & N ormF eed-F or w ar dAdd & N orm\n",
      "Bloc kLinearSoftmaxR epeat  f or number of \n",
      "encoder bloc ksPr obabilities\n",
      "E ncoder InputsT r ansf ormer E ncoder\n",
      "Figure 5: Diagram of the Transformer\n",
      "Encoder.In the case that one wants probabilities out of the tokens of a\n",
      "Transformer Encoder (as in masked language modeling for BERT\n",
      "[Devlin et al., 2019 ], which we’ll cover later), one applies a linear\n",
      "transformation to the output space followed by a softmax.\n",
      "Uses of the Transformer Encoder. A Transformer Encoder is great in\n",
      "contexts where you aren’t trying to generate text autoregressively\n",
      "(there’s no masking in the encoder so each position index can see\n",
      "the whole sequence,) and want strong representations for the whole\n",
      "sequence (again, possible because even the first token can see the\n",
      "whole future of the sequence when building its representation.)\n",
      "3.6Transformer Decoder\n",
      "To build a Transformer autoregressive language model, one uses a\n",
      "Transformer Decoder. These differ from Transformer Encoders simply\n",
      "by using future masking at each application of self-attention. This\n",
      "ensures that the informational constraint (no cheating by looking at\n",
      "the future!) holds throughout the architecture. We show a diagram\n",
      "of this architecture in Figure 4. Famous examples of this are GPT-\n",
      "2[Radford et al., 2019 ], GPT- 3[Brown et al., 2020 ] and BLOOM\n",
      "[Workshop et al., 2022 ].\n",
      "3.7Transformer Encoder-Decoder\n",
      "A Transformer encoder-decoder takes as input two sequences. Fig-\n",
      "ure6shows the whole encoder-decoder structure. The first sequence\n",
      "x1:nis passed through a Transformer Encoder to build contextual\n",
      "representations. The second sequence y1:mis encoded through a\n",
      "modified Transformer Decoder architecture in which cross-attention\n",
      "(which we haven’t yet defined!) is applied from the encoded repre-\n",
      "sentation of y1:mto the output of the Encoder. So, let’s take a quick\n",
      "detour to discuss cross-attention; it’s not too different from what\n",
      "we’ve already seen.\n",
      "-------------------------------------------------------------------------------------- 13\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 14\n",
      "Cross-Attention. Cross-attention uses one sequence to define the\n",
      "keys and values of self-attention, and another sequence to define\n",
      "the queries. You might think, hey wait, isn’t that just what attention\n",
      "always was before we got into this self-attention business? Yeah,\n",
      "pretty much. So if\n",
      "h(x)\n",
      "1:n=TransformerEncoder (w1:n), ( 33)\n",
      "and we have some intermediate representation h(y)of sequence y1:m,\n",
      "then we let the queries come from the decoder (the h(y)sequence)\n",
      "while the keys and values come from the encoder:\n",
      "qi=Qh(y)\n",
      "ii∈ {1, . . . , m} (34)\n",
      "kj=Kh(x)\n",
      "jj∈ {1, . . . , n} (35)\n",
      "vj=Vh(x)\n",
      "jj∈ {1, . . . , n}, ( 36)\n",
      "and compute the attention on q,k,vas we defined for self-attention.\n",
      "Note in Figure 6that in the Transformer Encoder-Decoder, cross-\n",
      "attention always applies to the output of the Transformer encoder.\n",
      "Uses of the encoder-decoder. An encoder-decoder is used when we’d\n",
      "like bidirectional context on something (like an article to summa-\n",
      "rize) to build strong represenations (i.e., each token can attend to\n",
      "all other tokens), but then generate an output according to an au-\n",
      "toregressive decomposition as we can with a decoder. While such\n",
      "an architecture has been found to provide better performance than\n",
      "decoder-only models at modest scale [ Raffel et al., 2020 ], it involves\n",
      "splitting parameters between encoder and decoder, and most of the\n",
      "largest Transformers are decoder-only.\n",
      ".1\n",
      "References\n",
      "[Ba et al., 2016 ]Ba, J. L., Kiros, J. R., and Hinton, G. E. ( 2016 ). Layer normalization.\n",
      "arXiv preprint arXiv: 1607 .06450 .\n",
      "[Bahdanau et al., 2014 ]Bahdanau, D., Cho, K., and Bengio, Y. ( 2014 ). Neural machine\n",
      "translation by jointly learning to align and translate. arXiv preprint arXiv: 1409 .0473 .\n",
      "[Baum and Petrie, 1966 ]Baum, L. E. and Petrie, T. ( 1966 ). Statistical inference for\n",
      "probabilistic functions of finite state markov chains. The Annals of Mathematical\n",
      "Statistics ,37(6):1554 –1563 .\n",
      "[Bengio et al., 2000 ]Bengio, Y., Ducharme, R., and Vincent, P . ( 2000 ). A neural\n",
      "probabilistic language model. Advances in neural information processing systems ,13.\n",
      "[Bengio et al., 2003 ]Bengio, Y., Ducharme, R., Vincent, P ., and Janvin, C. ( 2003 ). A\n",
      "neural probabilistic language model. J. Mach. Learn. Res. ,3:1137 –1155 .\n",
      "[Brown et al., 2020 ]Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhari-\n",
      "wal, P ., Neelakantan, A., Shyam, P ., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,\n",
      "-------------------------------------------------------------------------------------- 14\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 15\n",
      "Embedding sAdd P osition \n",
      "Embedding sMulti-Head \n",
      "A tt entionAdd & N ormF eed-F or w ar dAdd & N ormEmbedding sAdd P osition \n",
      "Embedding sBloc kMask ed Multi-\n",
      "Head A tt entionAdd & N ormMulti-Head \n",
      "A tt entionAdd & N ormF eed-F or w ar dLinearSoftmaxAdd & N orm\n",
      "Bloc kR epeat  f or number of \n",
      "encoder bloc ksR epeat  f or number of \n",
      "decoder bloc ks.\n",
      "\n",
      "A ttend onl y to output of \n",
      "last E ncoder Bloc k.Pr obabilities\n",
      "Decoder InputsE ncoder InputsT r ansf ormer E ncoder-Decoder\n",
      "Figure 6: A Transformer encoder-\n",
      "decoder.\n",
      "-------------------------------------------------------------------------------------- 15\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 16\n",
      "A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C.,\n",
      "Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C.,\n",
      "McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. ( 2020 ). Language models\n",
      "are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and\n",
      "Lin, H., editors, Advances in Neural Information Processing Systems , volume 33, pages\n",
      "1877 –1901 . Curran Associates, Inc.\n",
      "[Collobert et al., 2011 ]Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu,\n",
      "K., and Kuksa, P . P . ( 2011 ). Natural language processing (almost) from scratch. CoRR ,\n",
      "abs/1103 .0398 .\n",
      "[Cortes and Vapnik, 1995 ]Cortes, C. and Vapnik, V . ( 1995 ). Support-vector networks.\n",
      "Machine learning ,20(3):273–297.\n",
      "[Devlin et al., 2019 ]Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. ( 2019 ). BERT:\n",
      "Pre-training of deep bidirectional transformers for language understanding. In\n",
      "Proceedings of the 2019 Conference of the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Technologies, Volume 1(Long and Short\n",
      "Papers) , pages 4171 –4186 , Minneapolis, Minnesota. Association for Computational\n",
      "Linguistics.\n",
      "[Elman, 1990 ]Elman, J. L. ( 1990 ). Finding structure in time. Cognitive science ,14(2):179–\n",
      "211.\n",
      "[Fukushima and Miyake, 1982 ]Fukushima, K. and Miyake, S. ( 1982 ). Neocogni-\n",
      "tron: A self-organizing neural network model for a mechanism of visual pattern\n",
      "recognition. In Competition and cooperation in neural nets , pages 267–285. Springer.\n",
      "[Lafferty et al., 2001 ]Lafferty, J. D., McCallum, A., and Pereira, F. C. N. ( 2001 ).\n",
      "Conditional random fields: Probabilistic models for segmenting and labeling\n",
      "sequence data. In Proceedings of the Eighteenth International Conference on Machine\n",
      "Learning , ICML ’ 01, page 282–289, San Francisco, CA, USA. Morgan Kaufmann\n",
      "Publishers Inc.\n",
      "[LeCun et al., 1989 ]LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E.,\n",
      "Hubbard, W., and Jackel, L. D. ( 1989 ). Backpropagation applied to handwritten zip\n",
      "code recognition. Neural computation ,1(4):541–551.\n",
      "[Manning, 2022 ]Manning, C. D. ( 2022 ). Human Language Understanding & Reasoning.\n",
      "Daedalus ,151(2):127–138.\n",
      "[Mikolov et al., 2013 ]Mikolov, T., Chen, K., Corrado, G., and Dean, J. ( 2013 ). Efficient\n",
      "estimation of word representations in vector space. CoRR , abs/ 1301 .3781 .\n",
      "[Press et al., 2022 ]Press, O., Smith, N., and Lewis, M. ( 2022 ). Train short, test long:\n",
      "Attention with linear biases enables input length extrapolation. In International\n",
      "Conference on Learning Representations .\n",
      "[Radford et al., 2019 ]Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\n",
      "Sutskever, I. ( 2019 ). Language models are unsupervised multitask learners.\n",
      "[Raffel et al., 2020 ]Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,\n",
      "M., Zhou, Y., Li, W., and Liu, P . J. ( 2020 ). Exploring the limits of transfer learning\n",
      "with a unified text-to-text transformer. The Journal of Machine Learning Research ,\n",
      "21(1):5485 –5551 .\n",
      "[Rong, 2014 ]Rong, X. ( 2014 ). word 2vec parameter learning explained. CoRR ,\n",
      "abs/1411 .2738 .\n",
      "[Rumelhart et al., 1985 ]Rumelhart, D. E., Hinton, G. E., and Williams, R. J. ( 1985 ).\n",
      "Learning internal representations by error propagation. Technical report, California\n",
      "Univ San Diego La Jolla Inst for Cognitive Science.\n",
      "[Rumelhart et al., 1988 ]Rumelhart, D. E., Hinton, G. E., and Williams, R. J. ( 1988 ).\n",
      "Neurocomputing: Foundations of research. chapter Learning Representations by\n",
      "Back-propagating Errors, pages 696–699. MIT Press, Cambridge, MA, USA.\n",
      "[Schütze, 1992 ]Schütze, H. ( 1992 ). Dimensions of meaning. In Proceedings of the\n",
      "1992 ACM/IEEE Conference on Supercomputing , Supercomputing ’ 92, page 787–796,\n",
      "Washington, DC, USA. IEEE Computer Society Press.\n",
      "-------------------------------------------------------------------------------------- 16\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 17\n",
      "[Vaswani et al., 2017 ]Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\n",
      "Gomez, A. N., Kaiser, Ł., and Polosukhin, I. ( 2017 ). Attention is all you need.\n",
      "Advances in neural information processing systems ,30.\n",
      "[Workshop et al., 2022 ]Workshop, B., :, Scao, T. L., Fan, A., Akiki, C., Pavlick, E.,\n",
      "Ili´ c, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., Tow, J., Rush,\n",
      "A. M., Biderman, S., Webson, A., Ammanamanchi, P . S., Wang, T., Sagot, B., Muen-\n",
      "nighoff, N., del Moral, A. V ., Ruwase, O., Bawden, R., Bekman, S., McMillan-Major,\n",
      "A., Beltagy, I., Nguyen, H., Saulnier, L., Tan, S., Suarez, P . O., Sanh, V ., Laurençon,\n",
      "H., Jernite, Y., Launay, J., Mitchell, M., Raffel, C., Gokaslan, A., Simhi, A., Soroa,\n",
      "A., Aji, A. F., Alfassy, A., Rogers, A., Nitzav, A. K., Xu, C., Mou, C., Emezue, C.,\n",
      "Klamm, C., Leong, C., van Strien, D., Adelani, D. I., Radev, D., Ponferrada, E. G.,\n",
      "Levkovizh, E., Kim, E., Natan, E. B., De Toni, F., Dupont, G., Kruszewski, G., Pis-\n",
      "tilli, G., Elsahar, H., Benyamina, H., Tran, H., Yu, I., Abdulmumin, I., Johnson, I.,\n",
      "Gonzalez-Dios, I., de la Rosa, J., Chim, J., Dodge, J., Zhu, J., Chang, J., Frohberg, J.,\n",
      "Tobing, J., Bhattacharjee, J., Almubarak, K., Chen, K., Lo, K., Von Werra, L., Weber,\n",
      "L., Phan, L., allal, L. B., Tanguy, L., Dey, M., Muñoz, M. R., Masoud, M., Grandury,\n",
      "M., Šaško, M., Huang, M., Coavoux, M., Singh, M., Jiang, M. T.-J., Vu, M. C., Jauhar,\n",
      "M. A., Ghaleb, M., Subramani, N., Kassner, N., Khamis, N., Nguyen, O., Espejel,\n",
      "O., de Gibert, O., Villegas, P ., Henderson, P ., Colombo, P ., Amuok, P ., Lhoest, Q.,\n",
      "Harliman, R., Bommasani, R., López, R. L., Ribeiro, R., Osei, S., Pyysalo, S., Nagel,\n",
      "S., Bose, S., Muhammad, S. H., Sharma, S., Longpre, S., Nikpoor, S., Silberberg, S.,\n",
      "Pai, S., Zink, S., Torrent, T. T., Schick, T., Thrush, T., Danchev, V ., Nikoulina, V ., Laip-\n",
      "pala, V ., Lepercq, V ., Prabhu, V ., Alyafeai, Z., Talat, Z., Raja, A., Heinzerling, B., Si,\n",
      "C., Ta¸ sar, D. E., Salesky, E., Mielke, S. J., Lee, W. Y., Sharma, A., Santilli, A., Chaffin,\n",
      "A., Stiegler, A., Datta, D., Szczechla, E., Chhablani, G., Wang, H., Pandey, H., Stro-\n",
      "belt, H., Fries, J. A., Rozen, J., Gao, L., Sutawika, L., Bari, M. S., Al-shaibani, M. S.,\n",
      "Manica, M., Nayak, N., Teehan, R., Albanie, S., Shen, S., Ben-David, S., Bach, S. H.,\n",
      "Kim, T., Bers, T., Fevry, T., Neeraj, T., Thakker, U., Raunak, V ., Tang, X., Yong, Z.-X.,\n",
      "Sun, Z., Brody, S., Uri, Y., Tojarieh, H., Roberts, A., Chung, H. W., Tae, J., Phang, J.,\n",
      "Press, O., Li, C., Narayanan, D., Bourfoune, H., Casper, J., Rasley, J., Ryabinin, M.,\n",
      "Mishra, M., Zhang, M., Shoeybi, M., Peyrounette, M., Patry, N., Tazi, N., Sanseviero,\n",
      "O., von Platen, P ., Cornette, P ., Lavallée, P . F., Lacroix, R., Rajbhandari, S., Gandhi, S.,\n",
      "Smith, S., Requena, S., Patil, S., Dettmers, T., Baruwa, A., Singh, A., Cheveleva, A.,\n",
      "Ligozat, A.-L., Subramonian, A., Névéol, A., Lovering, C., Garrette, D., Tunuguntla,\n",
      "D., Reiter, E., Taktasheva, E., Voloshina, E., Bogdanov, E., Winata, G. I., Schoelkopf,\n",
      "H., Kalo, J.-C., Novikova, J., Forde, J. Z., Clive, J., Kasai, J., Kawamura, K., Hazan, L.,\n",
      "Carpuat, M., Clinciu, M., Kim, N., Cheng, N., Serikov, O., Antverg, O., van der Wal,\n",
      "O., Zhang, R., Zhang, R., Gehrmann, S., Mirkin, S., Pais, S., Shavrina, T., Scialom,\n",
      "T., Yun, T., Limisiewicz, T., Rieser, V ., Protasov, V ., Mikhailov, V ., Pruksachatkun,\n",
      "Y., Belinkov, Y., Bamberger, Z., Kasner, Z., Rueda, A., Pestana, A., Feizpour, A.,\n",
      "Khan, A., Faranak, A., Santos, A., Hevia, A., Unldreaj, A., Aghagol, A., Abdollahi,\n",
      "A., Tammour, A., HajiHosseini, A., Behroozi, B., Ajibade, B., Saxena, B., Ferrandis,\n",
      "C. M., Contractor, D., Lansky, D., David, D., Kiela, D., Nguyen, D. A., Tan, E., Bay-\n",
      "lor, E., Ozoani, E., Mirza, F., Ononiwu, F., Rezanejad, H., Jones, H., Bhattacharya, I.,\n",
      "Solaiman, I., Sedenko, I., Nejadgholi, I., Passmore, J., Seltzer, J., Sanz, J. B., Dutra,\n",
      "L., Samagaio, M., Elbadri, M., Mieskes, M., Gerchick, M., Akinlolu, M., McKenna,\n",
      "M., Qiu, M., Ghauri, M., Burynok, M., Abrar, N., Rajani, N., Elkott, N., Fahmy,\n",
      "N., Samuel, O., An, R., Kromann, R., Hao, R., Alizadeh, S., Shubber, S., Wang, S.,\n",
      "Roy, S., Viguier, S., Le, T., Oyebade, T., Le, T., Yang, Y., Nguyen, Z., Kashyap, A. R.,\n",
      "Palasciano, A., Callahan, A., Shukla, A., Miranda-Escalada, A., Singh, A., Beilharz,\n",
      "B., Wang, B., Brito, C., Zhou, C., Jain, C., Xu, C., Fourrier, C., Periñán, D. L., Molano,\n",
      "D., Yu, D., Manjavacas, E., Barth, F., Fuhrimann, F., Altay, G., Bayrak, G., Burns, G.,\n",
      "Vrabec, H. U., Bello, I., Dash, I., Kang, J., Giorgi, J., Golde, J., Posada, J. D., Sivara-\n",
      "man, K. R., Bulchandani, L., Liu, L., Shinzato, L., de Bykhovetz, M. H., Takeuchi, M.,\n",
      "Pàmies, M., Castillo, M. A., Nezhurina, M., Sänger, M., Samwald, M., Cullan, M.,\n",
      "Weinberg, M., De Wolf, M., Mihaljcic, M., Liu, M., Freidank, M., Kang, M., Seelam,\n",
      "N., Dahlberg, N., Broad, N. M., Muellner, N., Fung, P ., Haller, P ., Chandrasekhar,\n",
      "R., Eisenberg, R., Martin, R., Canalli, R., Su, R., Su, R., Cahyawijaya, S., Garda,\n",
      "S., Deshmukh, S. S., Mishra, S., Kiblawi, S., Ott, S., Sang-aroonsiri, S., Kumar, S.,\n",
      "Schweter, S., Bharati, S., Laud, T., Gigant, T., Kainuma, T., Kusa, W., Labrak, Y.,\n",
      "Bajaj, Y. S., Venkatraman, Y., Xu, Y., Xu, Y., Xu, Y., Tan, Z., Xie, Z., Ye, Z., Bras, M.,\n",
      "-------------------------------------------------------------------------------------- 17\n",
      "[draft ]note 10 :self -attention &transformers cs 224n :natural language processing\n",
      "with deep learning 18\n",
      "Belkada, Y., and Wolf, T. ( 2022 ). Bloom: A 176b-parameter open-access multilingual\n",
      "language model.\n",
      "[Xiong et al., 2020 ]Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang,\n",
      "H., Lan, Y., Wang, L., and Liu, T. ( 2020 ). On layer normalization in the transformer\n",
      "architecture. In International Conference on Machine Learning , pages 10524 –10533 .\n",
      "PMLR.\n",
      "[Xu et al., 2019 ]Xu, J., Sun, X., Zhang, Z., Zhao, G., and Lin, J. ( 2019 ). Understanding\n",
      "and improving layer normalization. Advances in Neural Information Processing Systems ,\n",
      "32.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    print('--------------------------------------------------------------------------------------', i)\n",
    "    print(data[i].page_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343aaa5-3e0d-4cf6-b9c9-3af5bcd55002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
