{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567ea94-42c1-413e-a644-0568d7187ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm(p,t,y_dat):  \n",
    "    \"\"\"\n",
    "    \n",
    "    Levenberg Marquardt curve-fitting: minimize sum of weighted squared residuals\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : initial guess of parameter values (n x 1)\n",
    "    t : independent variables (used as arg to lm_func) (m x 1)\n",
    "    y_dat : data to be fit by func(t,p) (m x 1)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p       : least-squares optimal estimate of the parameter values\n",
    "    redX2   : reduced Chi squared error criteria - should be close to 1\n",
    "    sigma_p : asymptotic standard error of the parameters\n",
    "    sigma_y : asymptotic standard error of the curve-fit\n",
    "    corr_p  : correlation matrix of the parameters\n",
    "    R_sq    : R-squared cofficient of multiple determination  \n",
    "    cvg_hst : convergence history (col 1: function calls, col 2: reduced chi-sq,\n",
    "              col 3 through n: parameter values). Row number corresponds to\n",
    "              iteration number.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    global iteration, func_calls\n",
    "    \n",
    "    # iteration counter\n",
    "    iteration  = 0\n",
    "    # running count of function evaluations\n",
    "    func_calls = 0\n",
    "    \n",
    "    # define eps (not available in python)\n",
    "    eps = 2**(-52)\n",
    "\n",
    "    # number of parameters\n",
    "    Npar   = len(p)\n",
    "    # number of data points\n",
    "    Npnt   = len(y_dat)\n",
    "    # previous set of parameters\n",
    "    p_old  = np.zeros((Npar,1))\n",
    "    # previous model, y_old = y_hat(t,p_old)\n",
    "    y_old  = np.zeros((Npnt,1))\n",
    "    # a really big initial Chi-sq value\n",
    "    X2     = 1e-3/eps\n",
    "    # a really big initial Chi-sq value\n",
    "    X2_old = 1e-3/eps\n",
    "    # Jacobian matrix\n",
    "    J      = np.zeros((Npnt,Npar))\n",
    "    # statistical degrees of freedom\n",
    "    DoF    = np.array([[Npnt - Npar + 1]])\n",
    "    \n",
    "    \n",
    "    if len(t) != len(y_dat):\n",
    "        print('The length of t must equal the length of y_dat!')\n",
    "        X2 = 0 \n",
    "        corr_p = 0 \n",
    "        sigma_p = 0 \n",
    "        sigma_y = 0\n",
    "        R_sq = 0\n",
    "\n",
    "    # weights or a scalar weight value ( weight >= 0 )\n",
    "    weight = 1/(y_dat.T@y_dat)\n",
    "    # fractional increment of 'p' for numerical derivatives\n",
    "    dp = [-0.001]      \n",
    "    # lower bounds for parameter values\n",
    "    p_min = -100*abs(p)  \n",
    "    # upper bounds for parameter values       \n",
    "    p_max = 100*abs(p)\n",
    "\n",
    "    MaxIter       = 1000        # maximum number of iterations\n",
    "    epsilon_1     = 1e-3        # convergence tolerance for gradient\n",
    "    epsilon_2     = 1e-3        # convergence tolerance for parameters\n",
    "    epsilon_4     = 1e-1        # determines acceptance of a L-M step\n",
    "    lambda_0      = 1e-2        # initial value of damping paramter, lambda\n",
    "    lambda_UP_fac = 11          # factor for increasing lambda\n",
    "    lambda_DN_fac = 9           # factor for decreasing lambda\n",
    "    Update_Type   = 1           # 1: Levenberg-Marquardt lambda update, 2: Quadratic update, 3: Nielsen's lambda update equations \n",
    "\n",
    "    if len(dp) == 1:\n",
    "        dp = dp*np.ones((Npar,1))\n",
    "\n",
    "    idx   = np.arange(len(dp))  # indices of the parameters to be fit\n",
    "    stop = 0                    # termination flag\n",
    "\n",
    "    # identical weights vector\n",
    "    if np.var(weight) == 0:         \n",
    "        weight = abs(weight)*np.ones((Npnt,1))        \n",
    "        print('Using uniform weights for error analysis')\n",
    "    else:\n",
    "        weight = abs(weight)\n",
    "\n",
    "    # initialize Jacobian with finite difference calculation\n",
    "    JtWJ,JtWdy,X2,y_hat,J = lm_matx(t,p_old,y_old,1,J,p,y_dat,weight,dp)\n",
    "    if np.abs(JtWdy).max() < epsilon_1:\n",
    "        print('*** Your Initial Guess is Extremely Close to Optimal ***')\n",
    "    \n",
    "    lambda_0 = np.atleast_2d([lambda_0])\n",
    "\n",
    "    # Marquardt: init'l lambda\n",
    "    if Update_Type == 1:\n",
    "        lambda_  = lambda_0\n",
    "    # Quadratic and Nielsen\n",
    "    else:\n",
    "        lambda_  = lambda_0 * max(np.diag(JtWJ))\n",
    "        nu=2\n",
    "    \n",
    "    # previous value of X2 \n",
    "    X2_old = X2\n",
    "    # initialize convergence history\n",
    "    cvg_hst = np.ones((MaxIter,Npar+2))   \n",
    "    \n",
    "    # -------- Start Main Loop ----------- #\n",
    "    while not stop and iteration <= MaxIter:\n",
    "        \n",
    "        iteration = iteration + 1\n",
    " \n",
    "        # incremental change in parameters\n",
    "        # Marquardt\n",
    "        if Update_Type == 1:\n",
    "            h = np.linalg.solve((JtWJ + lambda_*np.diag(np.diag(JtWJ)) ), JtWdy)  \n",
    "        # Quadratic and Nielsen\n",
    "        else:\n",
    "            h = np.linalg.solve(( JtWJ + lambda_*np.eye(Npar) ), JtWdy)\n",
    "\n",
    "        # update the [idx] elements\n",
    "        p_try = p + h[idx]\n",
    "        # apply constraints                             \n",
    "        p_try = np.minimum(np.maximum(p_min,p_try),p_max)       \n",
    "    \n",
    "        # residual error using p_try\n",
    "        delta_y = np.array([y_dat - lm_func(t,p_try)]).T\n",
    "        \n",
    "        # floating point error; break       \n",
    "        if not all(np.isfinite(delta_y)):                   \n",
    "          stop = 1\n",
    "          break     \n",
    "\n",
    "        func_calls = func_calls + 1\n",
    "        # Chi-squared error criteria\n",
    "        X2_try = delta_y.T @ ( delta_y * weight )\n",
    "        \n",
    "        # % Quadratic\n",
    "        if Update_Type == 2:                        \n",
    "          # One step of quadratic line update in the h direction for minimum X2\n",
    "          alpha =  np.divide(JtWdy.T @ h, ( (X2_try - X2)/2 + 2*JtWdy.T@h ))\n",
    "          h = alpha * h\n",
    "          \n",
    "          # % update only [idx] elements\n",
    "          p_try = p + h[idx]\n",
    "          # % apply constraints\n",
    "          p_try = np.minimum(np.maximum(p_min,p_try),p_max)         \n",
    "          \n",
    "          # % residual error using p_try\n",
    "          delta_y = y_dat - lm_func(t,p_try)     \n",
    "          func_calls = func_calls + 1\n",
    "          # % Chi-squared error criteria\n",
    "          X2_try = delta_y.T @ ( delta_y * weight )   \n",
    "  \n",
    "        rho = np.matmul( h.T @ (lambda_ * h + JtWdy),np.linalg.inv(X2 - X2_try))\n",
    "    \n",
    "        # it IS significantly better\n",
    "        if ( rho > epsilon_4 ):                         \n",
    "    \n",
    "            dX2 = X2 - X2_old\n",
    "            X2_old = X2\n",
    "            p_old = p\n",
    "            y_old = y_hat\n",
    "            # % accept p_try\n",
    "            p = p_try                        \n",
    "        \n",
    "            JtWJ,JtWdy,X2,y_hat,J = lm_matx(t,p_old,y_old,dX2,J,p,y_dat,weight,dp)\n",
    "            \n",
    "            # % decrease lambda ==> Gauss-Newton method\n",
    "            # % Levenberg\n",
    "            if Update_Type == 1:\n",
    "                lambda_ = max(lambda_/lambda_DN_fac,1.e-7)\n",
    "            # % Quadratic\n",
    "            elif Update_Type == 2:\n",
    "                lambda_ = max( lambda_/(1 + alpha) , 1.e-7 )\n",
    "            # % Nielsen\n",
    "            else:\n",
    "                lambda_ = lambda_*max( 1/3, 1-(2*rho-1)**3 )\n",
    "                nu = 2\n",
    "            \n",
    "        # it IS NOT better\n",
    "        else:                                           \n",
    "            # % do not accept p_try\n",
    "            X2 = X2_old\n",
    "    \n",
    "            if not np.remainder(iteration,2*Npar):            \n",
    "                JtWJ,JtWdy,dX2,y_hat,J = lm_matx(t,p_old,y_old,-1,J,p,y_dat,weight,dp)\n",
    "    \n",
    "            # % increase lambda  ==> gradient descent method\n",
    "            # % Levenberg\n",
    "            if Update_Type == 1:\n",
    "                lambda_ = min(lambda_*lambda_UP_fac,1.e7)\n",
    "            # % Quadratic\n",
    "            elif Update_Type == 2:\n",
    "                lambda_ = lambda_ + abs((X2_try - X2)/2/alpha)\n",
    "            # % Nielsen\n",
    "            else:\n",
    "                lambda_ = lambda_ * nu\n",
    "                nu = 2*nu\n",
    "\n",
    "        # update convergence history ... save _reduced_ Chi-square\n",
    "        cvg_hst[iteration-1,0] = func_calls\n",
    "        cvg_hst[iteration-1,1] = X2/DoF\n",
    "        \n",
    "        for i in range(Npar):\n",
    "            cvg_hst[iteration-1,i+2] = p.T[0][i]\n",
    "\n",
    "        if ( max(abs(JtWdy)) < epsilon_1  and  iteration > 2 ):\n",
    "          print('**** Convergence in r.h.s. (\"JtWdy\")  ****')\n",
    "          stop = 1\n",
    "    \n",
    "        if ( max(abs(h)/(abs(p)+1e-12)) < epsilon_2  and  iteration > 2 ): \n",
    "          print('**** Convergence in Parameters ****')\n",
    "          stop = 1\n",
    "    \n",
    "        if ( iteration == MaxIter ):\n",
    "          print('!! Maximum Number of Iterations Reached Without Convergence !!')\n",
    "          stop = 1\n",
    "\n",
    "        # --- End of Main Loop --- #\n",
    "        # --- convergence achieved, find covariance and confidence intervals\n",
    "\n",
    "    #  ---- Error Analysis ----\n",
    "    #  recompute equal weights for paramter error analysis\n",
    "    if np.var(weight) == 0:   \n",
    "        weight = DoF/(delta_y.T@delta_y) * np.ones((Npnt,1))\n",
    "      \n",
    "    # % reduced Chi-square                            \n",
    "    redX2 = X2 / DoF\n",
    "\n",
    "    JtWJ,JtWdy,X2,y_hat,J = lm_matx(t,p_old,y_old,-1,J,p,y_dat,weight,dp)\n",
    "\n",
    "    # standard error of parameters \n",
    "    covar_p = np.linalg.inv(JtWJ)\n",
    "    sigma_p = np.sqrt(np.diag(covar_p)) \n",
    "    error_p = sigma_p/p\n",
    "    \n",
    "    # standard error of the fit\n",
    "    sigma_y = np.zeros((Npnt,1))\n",
    "    for i in range(Npnt):\n",
    "        sigma_y[i,0] = J[i,:] @ covar_p @ J[i,:].T        \n",
    "\n",
    "    sigma_y = np.sqrt(sigma_y)\n",
    "\n",
    "    # parameter correlation matrix\n",
    "    corr_p = covar_p / [sigma_p@sigma_p.T]\n",
    "        \n",
    "    # coefficient of multiple determination\n",
    "    R_sq = np.correlate(y_dat, y_hat)\n",
    "    R_sq = 0        \n",
    "\n",
    "    # convergence history\n",
    "    cvg_hst = cvg_hst[:iteration,:]\n",
    "    \n",
    "    print('\\nLM fitting results:')\n",
    "    for i in range(Npar):\n",
    "        print('----------------------------- ')\n",
    "        print('parameter      = p%i' %(i+1))\n",
    "        print('fitted value   = %0.4f' % p[i,0])\n",
    "        print('standard error = %0.2f %%' % error_p[i,0])\n",
    "    \n",
    "    return p,redX2,sigma_p,sigma_y,corr_p,R_sq,cvg_hst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
